{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAukGfkyLkSlTFL5pfJRUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaik-vydyam-mohammad-khaleel/Campus-Placement-predictor/blob/main/License_Plate_Detection_and_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1onE782HNBt2",
        "outputId": "af5335a4-26fb-4194-c86f-d826c279a91e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKXhysZ95J0B",
        "outputId": "f1b07a26-9fcf-4d14-e8f8-317d7f8acca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Files extracted to: /content/plates_unzipped\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/datasets/Licplatesdetection_train.zip\"\n",
        "extract_path = \"/content/plates_unzipped\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Done! Files extracted to:\", extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "csv_path = \"/content/drive/MyDrive/datasets/Licplatesdetection_train.csv\"\n",
        "images_dir = \"/content/plates_unzipped/license_plates_detection_train\"\n",
        "output_dir = \"/content/dataset\"\n",
        "\n",
        "# Create YOLO folders\n",
        "for folder in [\"images/train\", \"images/val\", \"labels/train\", \"labels/val\"]:\n",
        "    os.makedirs(os.path.join(output_dir, folder), exist_ok=True)\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Get unique images\n",
        "unique_images = df['img_id'].unique()\n",
        "\n",
        "# Train/val split (80/20)\n",
        "train_imgs, val_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
        "\n",
        "def convert_to_yolo(row, img_w, img_h):\n",
        "    \"\"\"Convert VOC bbox format to YOLO format\"\"\"\n",
        "    xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "    x_center = ((xmin + xmax) / 2) / img_w\n",
        "    y_center = ((ymin + ymax) / 2) / img_h\n",
        "    width = (xmax - xmin) / img_w\n",
        "    height = (ymax - ymin) / img_h\n",
        "    return [0, x_center, y_center, width, height]  # class 0 = license_plate\n",
        "\n",
        "def process_images(img_list, split):\n",
        "    for img_name in img_list:\n",
        "        img_path = os.path.join(images_dir, img_name)\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\" Missing image: {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # Copy image\n",
        "        shutil.copy(img_path, os.path.join(output_dir, f\"images/{split}\", img_name))\n",
        "\n",
        "        # Load image size\n",
        "        img = cv2.imread(img_path)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # Get all bboxes for this image\n",
        "        labels = []\n",
        "        for _, row in df[df['img_id'] == img_name].iterrows():\n",
        "            yolo_box = convert_to_yolo(row, w, h)\n",
        "            labels.append(\" \".join(map(str, yolo_box)))\n",
        "\n",
        "        # Save label file\n",
        "        label_path = os.path.join(output_dir, f\"labels/{split}\", img_name.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
        "        with open(label_path, \"w\") as f:\n",
        "            f.write(\"\\n\".join(labels))\n",
        "\n",
        "# Process train and val sets\n",
        "process_images(train_imgs, \"train\")\n",
        "process_images(val_imgs, \"val\")\n",
        "\n",
        "print(\" YOLO dataset created at:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA79KDV4-IYQ",
        "outputId": "f34f73fb-c751-4f7f-a3bb-0b07bee73bd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " YOLO dataset created at: /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Path where dataset is stored\n",
        "dataset_path = \"/content/dataset\"\n",
        "yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
        "\n",
        "# Define YAML content\n",
        "data = {\n",
        "    \"train\": os.path.join(dataset_path, \"images/train\"),\n",
        "    \"val\": os.path.join(dataset_path, \"images/val\"),\n",
        "    \"nc\": 1,\n",
        "    \"names\": [\"license_plate\"]\n",
        "}\n",
        "\n",
        "# Save YAML file\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    yaml.dump(data, f, default_flow_style=False)\n",
        "\n",
        "print(f\"data.yaml created at: {yaml_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuFoIXNqVG6W",
        "outputId": "029598f8-ae2e-4276-e528-79ff8502b30a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.yaml created at: /content/dataset/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pre-trained YOLOv8 model (nano version is fastest)\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Train on your dataset\n",
        "model.train(\n",
        "    data=\"/content/dataset/data.yaml\",  # path to data.yaml\n",
        "    epochs=30,                          # increase if needed\n",
        "    imgsz=640,\n",
        "    batch=16\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh2zEmNPVj2-",
        "outputId": "9d2c22c4-e369-4c97-f7a2-eda848936d05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.194)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Ultralytics 8.3.194 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1823.7±633.7 MB/s, size: 116.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/labels/train.cache... 720 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 720/720 1.2Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 471.0±92.8 MB/s, size: 114.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/val.cache... 180 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 180/180 24.8Kit/s 0.0s\n",
            "Plotting labels to /content/runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train2\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.21G      1.254      2.452      1.126         19        640: 100% ━━━━━━━━━━━━ 45/45 3.0it/s 15.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.4it/s 1.8s\n",
            "                   all        180        180      0.993      0.756       0.98      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.71G      1.166      1.457      1.022         26        640: 100% ━━━━━━━━━━━━ 45/45 3.0it/s 14.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.7it/s 1.6s\n",
            "                   all        180        180      0.969      0.928      0.967      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.71G      1.159      1.235      1.024         32        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.5it/s 1.7s\n",
            "                   all        180        180      0.969      0.978      0.993      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.71G      1.115      1.047      1.012         33        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.5it/s 1.7s\n",
            "                   all        180        180      0.975      0.967      0.988      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.71G      1.124     0.9658      1.002         28        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.4it/s 1.7s\n",
            "                   all        180        180      0.977      0.983       0.99      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.71G      1.148      0.882      1.008         26        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.4it/s 2.5s\n",
            "                   all        180        180      0.967      0.966      0.987       0.71\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.71G      1.081     0.7907     0.9995         28        640: 100% ━━━━━━━━━━━━ 45/45 3.6it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.1it/s 2.9s\n",
            "                   all        180        180      0.977      0.989      0.993      0.719\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.71G      1.061     0.7569     0.9915         23        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.7it/s 2.2s\n",
            "                   all        180        180      0.983      0.981      0.994      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.71G      1.054     0.7253     0.9927         35        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.1it/s 1.5s\n",
            "                   all        180        180      0.973      0.987      0.994      0.737\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.71G      1.017     0.6923     0.9772         21        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.0it/s 1.5s\n",
            "                   all        180        180      0.952      0.983      0.993      0.765\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.71G     0.9678     0.6447     0.9649         26        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.7it/s 1.6s\n",
            "                   all        180        180      0.978       0.98      0.988      0.723\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.71G     0.9753      0.638     0.9486         23        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.6it/s 1.7s\n",
            "                   all        180        180      0.986      0.983      0.995      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.71G     0.9866     0.6284     0.9512         35        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.6it/s 1.6s\n",
            "                   all        180        180      0.981      0.989      0.994      0.759\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.71G     0.9668     0.6035     0.9452         31        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.0it/s 2.0s\n",
            "                   all        180        180      0.981          1      0.994      0.757\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      2.71G     0.9678     0.5914     0.9573         28        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.7it/s 1.6s\n",
            "                   all        180        180      0.987          1      0.994      0.773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      2.71G      0.904     0.5615     0.9318         29        640: 100% ━━━━━━━━━━━━ 45/45 3.3it/s 13.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.8it/s 1.6s\n",
            "                   all        180        180      0.992      0.994      0.995      0.768\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      2.71G     0.9181      0.551     0.9359         37        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.4it/s 1.8s\n",
            "                   all        180        180      0.997          1      0.995      0.781\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      2.71G     0.8829     0.5324     0.9271         22        640: 100% ━━━━━━━━━━━━ 45/45 3.5it/s 12.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.3it/s 2.6s\n",
            "                   all        180        180      0.997          1      0.995      0.789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      2.71G     0.9033     0.5332     0.9354         23        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.5it/s 2.4s\n",
            "                   all        180        180          1          1      0.995      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      2.71G     0.9049     0.5246     0.9273         36        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.1it/s 1.9s\n",
            "                   all        180        180      0.994      0.994      0.995      0.771\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      2.71G     0.8412     0.5022      0.916         15        640: 100% ━━━━━━━━━━━━ 45/45 3.1it/s 14.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.8it/s 1.6s\n",
            "                   all        180        180      0.989      0.989      0.991      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      2.71G     0.8292     0.4848     0.9141         16        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.4it/s 1.4s\n",
            "                   all        180        180      0.993      0.994      0.995       0.78\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      2.71G     0.8222     0.4633     0.9001         16        640: 100% ━━━━━━━━━━━━ 45/45 3.5it/s 13.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.8it/s 1.6s\n",
            "                   all        180        180      0.992          1      0.995      0.777\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      2.71G     0.8088     0.4528     0.9071         16        640: 100% ━━━━━━━━━━━━ 45/45 3.5it/s 12.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.8it/s 1.6s\n",
            "                   all        180        180      0.993          1      0.995      0.784\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      2.71G     0.8072     0.4371     0.9012         16        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.3it/s 1.4s\n",
            "                   all        180        180      0.999          1      0.995      0.789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      2.71G     0.7855     0.4228     0.8948         16        640: 100% ━━━━━━━━━━━━ 45/45 3.5it/s 12.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.7it/s 1.6s\n",
            "                   all        180        180      0.999          1      0.995      0.805\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      2.71G     0.7641     0.4193     0.8945         16        640: 100% ━━━━━━━━━━━━ 45/45 3.5it/s 12.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.7it/s 1.6s\n",
            "                   all        180        180      0.999          1      0.995      0.803\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      2.71G     0.7556     0.4163     0.8816         16        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.8it/s 1.6s\n",
            "                   all        180        180      0.999          1      0.995      0.796\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      2.71G     0.7349     0.3991     0.8863         15        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.1it/s 1.5s\n",
            "                   all        180        180      0.999          1      0.995      0.819\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      2.71G     0.7242     0.3899     0.8776         16        640: 100% ━━━━━━━━━━━━ 45/45 3.4it/s 13.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.0it/s 1.5s\n",
            "                   all        180        180      0.999          1      0.995      0.818\n",
            "\n",
            "30 epochs completed in 0.131 hours.\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.194 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.7it/s 3.5s\n",
            "                   all        180        180      0.999          1      0.995      0.819\n",
            "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e06a3473320>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.71856,     0.71856,      0.8256,     0.85484,      0.8849,     0.89809,      0.9152,     0.91805,      0.9297,     0.93638,     0.94241,      0.9451,      0.9471,     0.94869,     0.95013,     0.95133,     0.95721,     0.95788,     0.95831,     0.95874,     0.95917,      0.9596,     0.96005,\n",
              "            0.96073,      0.9614,     0.96208,     0.96334,     0.96519,     0.96532,     0.96545,     0.96558,     0.96571,     0.96583,     0.96596,     0.96609,     0.96622,     0.96635,     0.96647,      0.9666,     0.96673,     0.96686,     0.96698,     0.96711,     0.96724,     0.96737,     0.96749,\n",
              "            0.96762,     0.96777,     0.96831,     0.96885,     0.96938,     0.96992,     0.97039,     0.97059,     0.97079,     0.97098,     0.97118,     0.97138,     0.97158,     0.97178,     0.97197,     0.97217,     0.97237,     0.97257,     0.97276,     0.97296,     0.97313,     0.97329,     0.97346,\n",
              "            0.97362,     0.97378,     0.97395,     0.97411,     0.97428,     0.97444,      0.9746,     0.97477,     0.97493,     0.97509,     0.97526,     0.97542,     0.97558,     0.97566,     0.97572,     0.97577,     0.97583,     0.97589,     0.97595,       0.976,     0.97606,     0.97612,     0.97618,\n",
              "            0.97623,     0.97629,     0.97635,     0.97641,     0.97646,     0.97652,     0.97658,     0.97664,     0.97669,     0.97675,     0.97681,     0.97687,     0.97692,     0.97698,     0.97704,      0.9771,     0.97715,     0.97721,     0.97727,     0.97733,     0.97738,     0.97744,      0.9775,\n",
              "            0.97755,     0.97761,     0.97767,     0.97773,     0.97778,     0.97784,      0.9779,     0.97796,     0.97801,     0.97807,     0.97813,     0.97819,     0.97824,     0.98101,     0.98121,     0.98141,     0.98161,     0.98181,     0.98201,      0.9822,      0.9824,      0.9826,      0.9828,\n",
              "              0.983,     0.98319,     0.98339,     0.98359,     0.98676,     0.98727,     0.98777,     0.98828,     0.98878,     0.98907,     0.98919,     0.98931,     0.98943,     0.98954,     0.98966,     0.98978,     0.98989,     0.99001,     0.99013,     0.99025,     0.99036,     0.99048,      0.9906,\n",
              "            0.99071,     0.99083,     0.99095,     0.99106,     0.99118,      0.9913,     0.99141,     0.99153,     0.99165,     0.99191,     0.99261,     0.99332,     0.99402,     0.99448,      0.9945,     0.99452,     0.99453,     0.99455,     0.99457,     0.99458,      0.9946,     0.99462,     0.99464,\n",
              "            0.99465,     0.99467,     0.99469,      0.9947,     0.99472,     0.99474,     0.99476,     0.99477,     0.99479,     0.99481,     0.99483,     0.99484,     0.99486,     0.99488,     0.99489,     0.99491,     0.99493,     0.99495,     0.99496,     0.99498,       0.995,     0.99501,     0.99503,\n",
              "            0.99505,     0.99507,     0.99508,      0.9951,     0.99512,     0.99513,     0.99515,     0.99517,     0.99519,      0.9952,     0.99522,     0.99524,     0.99526,     0.99527,     0.99529,     0.99531,     0.99532,     0.99534,     0.99536,     0.99538,     0.99539,     0.99541,     0.99543,\n",
              "            0.99544,     0.99546,     0.99548,      0.9955,     0.99551,     0.99553,     0.99555,     0.99556,     0.99558,      0.9956,     0.99562,     0.99563,     0.99565,     0.99567,     0.99568,      0.9957,     0.99572,     0.99574,     0.99575,     0.99577,     0.99579,      0.9958,     0.99582,\n",
              "            0.99584,     0.99586,     0.99587,     0.99589,     0.99591,     0.99592,     0.99594,     0.99596,     0.99598,     0.99599,     0.99601,     0.99603,     0.99604,     0.99606,     0.99608,      0.9961,     0.99611,     0.99613,     0.99615,     0.99617,     0.99618,      0.9962,     0.99622,\n",
              "            0.99623,     0.99625,     0.99627,     0.99629,      0.9963,     0.99632,     0.99634,     0.99635,     0.99637,     0.99639,     0.99641,     0.99642,     0.99644,     0.99646,     0.99647,     0.99649,     0.99651,     0.99653,     0.99654,     0.99656,     0.99658,     0.99659,     0.99661,\n",
              "            0.99663,     0.99665,     0.99666,     0.99668,      0.9967,     0.99671,     0.99673,     0.99675,     0.99677,     0.99678,      0.9968,     0.99682,     0.99683,     0.99685,     0.99687,     0.99689,      0.9969,     0.99692,     0.99694,     0.99695,     0.99697,     0.99699,       0.997,\n",
              "            0.99702,     0.99704,     0.99706,     0.99707,     0.99709,     0.99711,     0.99712,     0.99714,     0.99716,     0.99718,     0.99719,     0.99721,     0.99723,     0.99724,     0.99725,     0.99726,     0.99727,     0.99727,     0.99728,     0.99729,      0.9973,     0.99731,     0.99732,\n",
              "            0.99733,     0.99734,     0.99735,     0.99736,     0.99737,     0.99738,     0.99739,     0.99739,      0.9974,     0.99741,     0.99742,     0.99743,     0.99744,     0.99745,     0.99746,     0.99747,     0.99748,     0.99749,      0.9975,     0.99751,     0.99751,     0.99752,     0.99753,\n",
              "            0.99754,     0.99755,     0.99756,     0.99757,     0.99758,     0.99759,      0.9976,     0.99761,     0.99762,     0.99762,     0.99763,     0.99764,     0.99765,     0.99766,     0.99767,     0.99768,     0.99769,      0.9977,     0.99771,     0.99772,     0.99773,     0.99774,     0.99774,\n",
              "            0.99775,     0.99776,     0.99777,     0.99778,     0.99779,      0.9978,     0.99781,     0.99782,     0.99783,     0.99784,     0.99785,     0.99785,     0.99786,     0.99787,     0.99788,     0.99789,      0.9979,     0.99791,     0.99792,     0.99793,     0.99794,     0.99795,     0.99796,\n",
              "            0.99797,     0.99797,     0.99798,     0.99799,       0.998,     0.99801,     0.99802,     0.99803,     0.99804,     0.99805,     0.99806,     0.99807,     0.99808,     0.99808,     0.99809,      0.9981,     0.99811,     0.99812,     0.99813,     0.99814,     0.99815,     0.99816,     0.99817,\n",
              "            0.99818,     0.99819,      0.9982,      0.9982,     0.99821,     0.99822,     0.99823,     0.99824,     0.99825,     0.99826,     0.99827,     0.99828,     0.99829,      0.9983,     0.99831,     0.99831,     0.99832,     0.99833,     0.99834,     0.99835,     0.99836,     0.99837,     0.99838,\n",
              "            0.99839,      0.9984,     0.99841,     0.99842,     0.99843,     0.99843,     0.99844,     0.99845,     0.99846,     0.99847,     0.99848,     0.99849,      0.9985,     0.99851,     0.99852,     0.99853,     0.99854,     0.99854,     0.99855,     0.99856,     0.99857,     0.99858,     0.99859,\n",
              "             0.9986,     0.99861,     0.99862,     0.99863,     0.99864,     0.99865,     0.99865,     0.99866,     0.99867,     0.99868,     0.99869,      0.9987,     0.99871,     0.99872,     0.99873,     0.99874,     0.99875,     0.99876,     0.99877,     0.99877,     0.99878,     0.99879,      0.9988,\n",
              "            0.99881,     0.99882,     0.99883,     0.99884,     0.99885,     0.99886,     0.99887,     0.99888,     0.99888,     0.99889,      0.9989,     0.99891,     0.99892,     0.99893,     0.99894,     0.99895,     0.99896,     0.99897,     0.99898,     0.99899,     0.99899,       0.999,     0.99901,\n",
              "            0.99902,     0.99903,     0.99904,     0.99905,     0.99906,     0.99907,     0.99908,     0.99909,      0.9991,      0.9991,     0.99911,     0.99912,     0.99913,     0.99914,     0.99915,     0.99916,     0.99917,     0.99918,     0.99919,      0.9992,     0.99921,     0.99922,     0.99922,\n",
              "            0.99923,     0.99924,     0.99925,     0.99926,     0.99927,     0.99928,     0.99929,      0.9993,     0.99931,     0.99932,     0.99933,     0.99933,     0.99934,     0.99935,     0.99936,     0.99937,     0.99938,     0.99939,      0.9994,     0.99941,     0.99942,     0.99943,     0.99944,\n",
              "            0.99944,     0.99945,     0.99946,     0.99947,     0.99948,     0.99949,      0.9995,     0.99951,     0.99952,     0.99953,     0.99954,     0.99955,     0.99955,     0.99956,     0.99957,     0.99958,     0.99959,      0.9996,     0.99961,     0.99962,     0.99963,     0.99964,     0.99965,\n",
              "            0.99966,     0.99966,     0.99967,     0.99968,     0.99969,      0.9997,     0.99971,     0.99972,     0.99973,     0.99974,     0.99975,     0.99976,     0.99977,     0.99977,     0.99978,     0.99979,      0.9998,     0.99981,     0.99982,     0.99983,     0.99984,     0.99985,     0.99986,\n",
              "            0.99987,     0.99988,     0.99988,     0.99989,      0.9999,     0.99991,     0.99992,     0.99993,     0.99994,     0.99995,     0.99996,     0.99997,     0.99998,     0.99999,     0.99999,     0.99998,     0.99995,     0.99991,     0.99987,     0.99983,     0.99979,     0.99976,     0.99972,\n",
              "            0.99968,     0.99964,      0.9996,     0.99956,     0.99953,     0.99949,     0.99945,     0.99941,     0.99937,     0.99933,      0.9993,     0.99926,     0.99922,     0.99918,     0.99914,      0.9991,     0.99907,     0.99903,     0.99899,     0.99895,     0.99891,     0.99887,     0.99884,\n",
              "             0.9988,     0.99876,     0.99872,     0.99868,     0.99865,     0.99861,     0.99857,     0.99853,     0.99849,     0.99845,     0.99842,     0.99838,     0.99834,      0.9983,     0.99826,     0.99822,     0.99818,     0.99815,     0.99811,     0.99807,     0.99803,     0.99799,     0.99795,\n",
              "            0.99792,     0.99788,     0.99784,      0.9978,     0.99776,     0.99772,     0.99769,     0.99765,     0.99761,     0.99757,     0.99753,     0.99749,     0.99746,     0.99742,     0.99738,     0.99734,      0.9973,     0.99726,     0.99723,     0.99716,     0.99709,     0.99702,     0.99694,\n",
              "            0.99687,      0.9968,     0.99673,     0.99665,     0.99658,     0.99651,     0.99644,     0.99636,     0.99629,     0.99622,     0.99615,     0.99607,       0.996,     0.99593,     0.99586,     0.99578,     0.99571,     0.99564,     0.99556,     0.99549,     0.99542,     0.99535,     0.99527,\n",
              "             0.9952,     0.99513,     0.99506,     0.99498,     0.99491,     0.99484,     0.99476,     0.99469,     0.99462,     0.99455,     0.99447,     0.99405,     0.99213,     0.99149,     0.99133,     0.99118,     0.99103,     0.99088,     0.99072,     0.99057,     0.99042,     0.99027,     0.99011,\n",
              "            0.98996,     0.98981,     0.98965,      0.9895,     0.98935,     0.98919,     0.98904,     0.98889,     0.98861,     0.98777,     0.98694,      0.9861,     0.98579,     0.98563,     0.98548,     0.98532,     0.98516,       0.985,     0.98484,     0.98468,     0.98452,     0.98437,     0.98421,\n",
              "            0.98405,     0.98389,     0.98373,     0.98357,     0.98341,     0.98325,     0.98309,     0.98285,     0.98257,     0.98229,       0.982,     0.98172,     0.98144,     0.98116,     0.98088,      0.9806,     0.98032,     0.97945,     0.97797,     0.97383,     0.97283,     0.97183,     0.97052,\n",
              "            0.96901,     0.96419,     0.96137,     0.95867,     0.95743,     0.95619,     0.95495,     0.95371,     0.95246,     0.95121,     0.94801,     0.94666,     0.94587,     0.94508,     0.94429,     0.94349,      0.9427,      0.9419,     0.94058,     0.93435,     0.93343,      0.9325,     0.93113,\n",
              "            0.92822,     0.92658,     0.92128,     0.91868,     0.91785,     0.91701,     0.91618,     0.91307,     0.90437,     0.90144,     0.89915,     0.88892,     0.87843,     0.87468,     0.86384,     0.85645,     0.84593,     0.84403,     0.84117,     0.83232,     0.81796,     0.80895,     0.78974,\n",
              "             0.7772,     0.76577,     0.74699,     0.73798,     0.73286,      0.7011,     0.68184,     0.66681,     0.65172,     0.63107,     0.62547,     0.61867,     0.61504,     0.61139,     0.60115,     0.58076,     0.55773,     0.52795,     0.51395,     0.50574,     0.50392,      0.5021,     0.50027,\n",
              "            0.48503,     0.46943,     0.44261,     0.43174,     0.41449,     0.38204,     0.37583,     0.37035,     0.35508,      0.3456,     0.34008,     0.32329,     0.31246,     0.28249,     0.25067,     0.24818,     0.24568,     0.24135,     0.22095,     0.21616,     0.21414,     0.21212,      0.2101,\n",
              "            0.18688,     0.15437,     0.14435,     0.13327,     0.13043,     0.12758,     0.12436,     0.11766,     0.10365,     0.10108,    0.098514,    0.095936,    0.090201,    0.084489,    0.082164,    0.079834,    0.077499,    0.075157,    0.073443,    0.071816,    0.070186,    0.068553,    0.066918,\n",
              "           0.065279,    0.062885,    0.059828,    0.056761,    0.052758,    0.043192,    0.041208,    0.039219,    0.037227,     0.03523,     0.03323,    0.016177,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.56075,     0.56075,       0.703,     0.74648,     0.79356,     0.81502,     0.84366,     0.84852,     0.86863,     0.88036,     0.89109,     0.89591,     0.89952,     0.90238,       0.905,     0.90717,     0.91793,     0.91916,     0.91996,     0.92075,     0.92155,     0.92234,     0.92318,\n",
              "            0.92443,     0.92567,     0.92692,     0.92927,     0.93273,     0.93297,     0.93321,     0.93345,     0.93369,     0.93393,     0.93417,      0.9344,     0.93464,     0.93488,     0.93512,     0.93536,      0.9356,     0.93584,     0.93608,     0.93632,     0.93656,     0.93679,     0.93703,\n",
              "            0.93727,     0.93755,     0.93856,     0.93958,     0.94059,      0.9416,     0.94248,     0.94286,     0.94323,     0.94361,     0.94398,     0.94435,     0.94473,      0.9451,     0.94548,     0.94585,     0.94622,      0.9466,     0.94697,     0.94735,     0.94766,     0.94797,     0.94828,\n",
              "             0.9486,     0.94891,     0.94922,     0.94953,     0.94984,     0.95015,     0.95046,     0.95078,     0.95109,      0.9514,     0.95171,     0.95202,     0.95233,     0.95247,     0.95258,     0.95269,      0.9528,     0.95291,     0.95302,     0.95313,     0.95324,     0.95335,     0.95346,\n",
              "            0.95357,     0.95368,     0.95379,      0.9539,     0.95401,     0.95412,     0.95423,     0.95434,     0.95445,     0.95456,     0.95467,     0.95478,     0.95489,       0.955,     0.95511,     0.95522,     0.95533,     0.95544,     0.95555,     0.95566,     0.95577,     0.95588,     0.95599,\n",
              "             0.9561,     0.95621,     0.95631,     0.95642,     0.95653,     0.95664,     0.95675,     0.95686,     0.95697,     0.95708,     0.95719,      0.9573,     0.95741,     0.96274,     0.96312,      0.9635,     0.96388,     0.96427,     0.96465,     0.96503,     0.96541,     0.96579,     0.96618,\n",
              "            0.96656,     0.96694,     0.96732,     0.96771,     0.97387,     0.97485,     0.97584,     0.97682,     0.97781,     0.97838,     0.97861,     0.97884,     0.97907,      0.9793,     0.97953,     0.97976,     0.97999,     0.98022,     0.98045,     0.98068,     0.98091,     0.98114,     0.98137,\n",
              "             0.9816,     0.98183,     0.98206,     0.98229,     0.98252,     0.98275,     0.98297,      0.9832,     0.98343,     0.98395,     0.98534,     0.98672,      0.9881,     0.98902,     0.98906,     0.98909,     0.98912,     0.98916,     0.98919,     0.98923,     0.98926,      0.9893,     0.98933,\n",
              "            0.98936,      0.9894,     0.98943,     0.98947,      0.9895,     0.98953,     0.98957,      0.9896,     0.98964,     0.98967,      0.9897,     0.98974,     0.98977,     0.98981,     0.98984,     0.98987,     0.98991,     0.98994,     0.98998,     0.99001,     0.99004,     0.99008,     0.99011,\n",
              "            0.99015,     0.99018,     0.99021,     0.99025,     0.99028,     0.99032,     0.99035,     0.99038,     0.99042,     0.99045,     0.99049,     0.99052,     0.99055,     0.99059,     0.99062,     0.99066,     0.99069,     0.99073,     0.99076,     0.99079,     0.99083,     0.99086,      0.9909,\n",
              "            0.99093,     0.99096,       0.991,     0.99103,     0.99107,      0.9911,     0.99113,     0.99117,      0.9912,     0.99124,     0.99127,      0.9913,     0.99134,     0.99137,     0.99141,     0.99144,     0.99147,     0.99151,     0.99154,     0.99158,     0.99161,     0.99164,     0.99168,\n",
              "            0.99171,     0.99175,     0.99178,     0.99181,     0.99185,     0.99188,     0.99192,     0.99195,     0.99198,     0.99202,     0.99205,     0.99209,     0.99212,     0.99216,     0.99219,     0.99222,     0.99226,     0.99229,     0.99233,     0.99236,     0.99239,     0.99243,     0.99246,\n",
              "             0.9925,     0.99253,     0.99256,      0.9926,     0.99263,     0.99267,      0.9927,     0.99273,     0.99277,      0.9928,     0.99284,     0.99287,      0.9929,     0.99294,     0.99297,     0.99301,     0.99304,     0.99307,     0.99311,     0.99314,     0.99318,     0.99321,     0.99324,\n",
              "            0.99328,     0.99331,     0.99335,     0.99338,     0.99341,     0.99345,     0.99348,     0.99352,     0.99355,     0.99359,     0.99362,     0.99365,     0.99369,     0.99372,     0.99376,     0.99379,     0.99382,     0.99386,     0.99389,     0.99393,     0.99396,     0.99399,     0.99403,\n",
              "            0.99406,      0.9941,     0.99413,     0.99416,      0.9942,     0.99423,     0.99427,      0.9943,     0.99433,     0.99437,      0.9944,     0.99444,     0.99447,     0.99449,     0.99451,     0.99453,     0.99455,     0.99456,     0.99458,      0.9946,     0.99462,     0.99464,     0.99466,\n",
              "            0.99467,     0.99469,     0.99471,     0.99473,     0.99475,     0.99477,     0.99478,      0.9948,     0.99482,     0.99484,     0.99486,     0.99488,     0.99489,     0.99491,     0.99493,     0.99495,     0.99497,     0.99499,       0.995,     0.99502,     0.99504,     0.99506,     0.99508,\n",
              "             0.9951,     0.99511,     0.99513,     0.99515,     0.99517,     0.99519,     0.99521,     0.99522,     0.99524,     0.99526,     0.99528,      0.9953,     0.99532,     0.99533,     0.99535,     0.99537,     0.99539,     0.99541,     0.99543,     0.99544,     0.99546,     0.99548,      0.9955,\n",
              "            0.99552,     0.99554,     0.99555,     0.99557,     0.99559,     0.99561,     0.99563,     0.99565,     0.99566,     0.99568,      0.9957,     0.99572,     0.99574,     0.99576,     0.99577,     0.99579,     0.99581,     0.99583,     0.99585,     0.99587,     0.99588,      0.9959,     0.99592,\n",
              "            0.99594,     0.99596,     0.99598,     0.99599,     0.99601,     0.99603,     0.99605,     0.99607,     0.99609,      0.9961,     0.99612,     0.99614,     0.99616,     0.99618,      0.9962,     0.99621,     0.99623,     0.99625,     0.99627,     0.99629,     0.99631,     0.99632,     0.99634,\n",
              "            0.99636,     0.99638,      0.9964,     0.99642,     0.99643,     0.99645,     0.99647,     0.99649,     0.99651,     0.99653,     0.99654,     0.99656,     0.99658,      0.9966,     0.99662,     0.99664,     0.99665,     0.99667,     0.99669,     0.99671,     0.99673,     0.99675,     0.99676,\n",
              "            0.99678,      0.9968,     0.99682,     0.99684,     0.99686,     0.99687,     0.99689,     0.99691,     0.99693,     0.99695,     0.99697,     0.99698,       0.997,     0.99702,     0.99704,     0.99706,     0.99708,     0.99709,     0.99711,     0.99713,     0.99715,     0.99717,     0.99719,\n",
              "             0.9972,     0.99722,     0.99724,     0.99726,     0.99728,      0.9973,     0.99731,     0.99733,     0.99735,     0.99737,     0.99739,      0.9974,     0.99742,     0.99744,     0.99746,     0.99748,      0.9975,     0.99751,     0.99753,     0.99755,     0.99757,     0.99759,     0.99761,\n",
              "            0.99762,     0.99764,     0.99766,     0.99768,      0.9977,     0.99772,     0.99773,     0.99775,     0.99777,     0.99779,     0.99781,     0.99783,     0.99784,     0.99786,     0.99788,      0.9979,     0.99792,     0.99794,     0.99795,     0.99797,     0.99799,     0.99801,     0.99803,\n",
              "            0.99805,     0.99806,     0.99808,      0.9981,     0.99812,     0.99814,     0.99816,     0.99817,     0.99819,     0.99821,     0.99823,     0.99825,     0.99827,     0.99828,      0.9983,     0.99832,     0.99834,     0.99836,     0.99838,     0.99839,     0.99841,     0.99843,     0.99845,\n",
              "            0.99847,     0.99849,      0.9985,     0.99852,     0.99854,     0.99856,     0.99858,      0.9986,     0.99861,     0.99863,     0.99865,     0.99867,     0.99869,     0.99871,     0.99872,     0.99874,     0.99876,     0.99878,      0.9988,     0.99882,     0.99883,     0.99885,     0.99887,\n",
              "            0.99889,     0.99891,     0.99893,     0.99894,     0.99896,     0.99898,       0.999,     0.99902,     0.99904,     0.99905,     0.99907,     0.99909,     0.99911,     0.99913,     0.99915,     0.99916,     0.99918,      0.9992,     0.99922,     0.99924,     0.99926,     0.99927,     0.99929,\n",
              "            0.99931,     0.99933,     0.99935,     0.99937,     0.99938,      0.9994,     0.99942,     0.99944,     0.99946,     0.99948,     0.99949,     0.99951,     0.99953,     0.99955,     0.99957,     0.99959,      0.9996,     0.99962,     0.99964,     0.99966,     0.99968,      0.9997,     0.99971,\n",
              "            0.99973,     0.99975,     0.99977,     0.99979,     0.99981,     0.99982,     0.99984,     0.99986,     0.99988,      0.9999,     0.99992,     0.99993,     0.99995,     0.99997,     0.99999,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99997,     0.99989,     0.99982,     0.99974,     0.99966,     0.99959,     0.99951,     0.99943,\n",
              "            0.99936,     0.99928,      0.9992,     0.99913,     0.99905,     0.99898,      0.9989,     0.99882,     0.99875,     0.99867,     0.99859,     0.99852,     0.99844,     0.99836,     0.99829,     0.99821,     0.99813,     0.99806,     0.99798,     0.99791,     0.99783,     0.99775,     0.99768,\n",
              "             0.9976,     0.99752,     0.99745,     0.99737,     0.99729,     0.99722,     0.99714,     0.99706,     0.99699,     0.99691,     0.99684,     0.99676,     0.99668,     0.99661,     0.99653,     0.99645,     0.99638,      0.9963,     0.99622,     0.99615,     0.99607,     0.99599,     0.99592,\n",
              "            0.99584,     0.99576,     0.99569,     0.99561,     0.99554,     0.99546,     0.99538,     0.99531,     0.99523,     0.99515,     0.99508,       0.995,     0.99492,     0.99485,     0.99477,     0.99469,     0.99462,     0.99454,     0.99447,     0.99434,      0.9942,     0.99405,     0.99391,\n",
              "            0.99376,     0.99362,     0.99347,     0.99333,     0.99319,     0.99304,      0.9929,     0.99275,     0.99261,     0.99247,     0.99232,     0.99218,     0.99203,     0.99189,     0.99174,      0.9916,     0.99146,     0.99131,     0.99117,     0.99102,     0.99088,     0.99074,     0.99059,\n",
              "            0.99045,      0.9903,     0.99016,     0.99001,     0.98987,     0.98973,     0.98958,     0.98944,     0.98929,     0.98915,     0.98901,     0.98817,     0.98438,     0.98312,     0.98282,     0.98252,     0.98222,     0.98192,     0.98162,     0.98132,     0.98102,     0.98072,     0.98042,\n",
              "            0.98012,     0.97982,     0.97952,     0.97922,     0.97892,     0.97862,     0.97832,     0.97802,     0.97747,     0.97584,     0.97422,     0.97259,     0.97198,     0.97168,     0.97137,     0.97106,     0.97075,     0.97044,     0.97014,     0.96983,     0.96952,     0.96921,     0.96891,\n",
              "             0.9686,     0.96829,     0.96798,     0.96767,     0.96737,     0.96706,     0.96675,     0.96627,     0.96573,     0.96519,     0.96465,      0.9641,     0.96356,     0.96302,     0.96248,     0.96193,     0.96139,     0.95974,     0.95689,     0.94899,     0.94709,     0.94519,     0.94272,\n",
              "            0.93987,     0.93085,     0.92561,     0.92062,     0.91834,     0.91607,     0.91379,     0.91151,     0.90923,     0.90695,     0.90116,     0.89872,      0.8973,     0.89587,     0.89445,     0.89303,      0.8916,     0.89018,     0.88783,     0.87679,     0.87517,     0.87354,     0.87114,\n",
              "            0.86605,     0.86321,     0.85405,     0.84959,     0.84817,     0.84674,     0.84532,     0.84005,     0.82544,     0.82057,     0.81677,     0.80004,     0.78322,     0.77727,     0.76032,     0.74893,       0.733,     0.73015,     0.72588,      0.7128,     0.69199,     0.67919,     0.65254,\n",
              "             0.6356,     0.62044,     0.59615,     0.58476,     0.57835,     0.53976,     0.51726,     0.50016,     0.48337,     0.46099,     0.45504,     0.44788,     0.44409,     0.44029,     0.42974,      0.4092,      0.3867,     0.35865,     0.34585,     0.33846,     0.33683,      0.3352,     0.33358,\n",
              "            0.32016,      0.3067,      0.2842,     0.27529,     0.26142,     0.23613,      0.2314,     0.22726,     0.21587,      0.2089,     0.20488,     0.19281,     0.18515,     0.16448,      0.1433,     0.14167,     0.14004,     0.13724,      0.1242,     0.12118,     0.11991,     0.11865,     0.11738,\n",
              "            0.10307,    0.083639,    0.077792,    0.071393,    0.069766,    0.068139,    0.066305,    0.062509,    0.054656,    0.053232,    0.051809,    0.050385,    0.047231,    0.044108,    0.042842,    0.041577,    0.040311,    0.039046,    0.038121,    0.037245,    0.036369,    0.035493,    0.034617,\n",
              "           0.033741,    0.032463,    0.030836,    0.029209,    0.027094,    0.022073,    0.021037,    0.020002,    0.018966,    0.017931,    0.016896,   0.0081545,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.8361579715862841)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.81851])\n",
              "names: {0: 'license_plate'}\n",
              "nt_per_class: array([180])\n",
              "nt_per_image: array([180])\n",
              "results_dict: {'metrics/precision(B)': 0.9994392916043581, 'metrics/recall(B)': 1.0, 'metrics/mAP50(B)': 0.995, 'metrics/mAP50-95(B)': 0.8185088573180934, 'fitness': 0.8361579715862841}\n",
              "save_dir: PosixPath('/content/runs/detect/train2')\n",
              "speed: {'preprocess': 0.19672013333496982, 'inference': 2.187705566667672, 'loss': 0.0003806055575397396, 'postprocess': 5.383555777778939}\n",
              "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load trained model\n",
        "trained_model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
        "\n",
        "# Run predictions\n",
        "results = trained_model.predict(\n",
        "    source=\"/content/dataset/images/val\",\n",
        "    conf=0.25,\n",
        "    save=True\n",
        ")\n",
        "\n",
        "# Get the folder where predictions are saved\n",
        "print(\" Predictions saved in:\", trained_model.predictor.save_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71WrP2EpbkX0",
        "outputId": "77a6909d-9bbf-4aa0-b04b-49536cf91dba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/180 /content/dataset/images/val/103.jpg: 480x640 1 license_plate, 53.5ms\n",
            "image 2/180 /content/dataset/images/val/12.jpg: 384x640 1 license_plate, 43.1ms\n",
            "image 3/180 /content/dataset/images/val/121.jpg: 384x640 1 license_plate, 6.3ms\n",
            "image 4/180 /content/dataset/images/val/126.jpg: 640x480 1 license_plate, 43.2ms\n",
            "image 5/180 /content/dataset/images/val/127.jpg: 480x640 1 license_plate, 7.7ms\n",
            "image 6/180 /content/dataset/images/val/129.jpg: 640x480 1 license_plate, 6.8ms\n",
            "image 7/180 /content/dataset/images/val/134.jpg: 384x640 1 license_plate, 6.6ms\n",
            "image 8/180 /content/dataset/images/val/139.jpg: 640x480 1 license_plate, 6.9ms\n",
            "image 9/180 /content/dataset/images/val/143.jpg: 480x640 1 license_plate, 7.2ms\n",
            "image 10/180 /content/dataset/images/val/148.jpg: 352x640 1 license_plate, 44.5ms\n",
            "image 11/180 /content/dataset/images/val/153.jpg: 640x480 1 license_plate, 7.6ms\n",
            "image 12/180 /content/dataset/images/val/156.jpg: 384x640 1 license_plate, 6.7ms\n",
            "image 13/180 /content/dataset/images/val/158.jpg: 384x640 1 license_plate, 6.1ms\n",
            "image 14/180 /content/dataset/images/val/159.jpg: 640x480 1 license_plate, 6.8ms\n",
            "image 15/180 /content/dataset/images/val/16.jpg: 480x640 1 license_plate, 7.1ms\n",
            "image 16/180 /content/dataset/images/val/162.jpg: 480x640 1 license_plate, 6.3ms\n",
            "image 17/180 /content/dataset/images/val/164.jpg: 640x480 1 license_plate, 6.9ms\n",
            "image 18/180 /content/dataset/images/val/168.jpg: 480x640 1 license_plate, 6.8ms\n",
            "image 19/180 /content/dataset/images/val/169.jpg: 480x640 1 license_plate, 6.1ms\n",
            "image 20/180 /content/dataset/images/val/17.jpg: 384x640 1 license_plate, 6.9ms\n",
            "image 21/180 /content/dataset/images/val/173.jpg: 640x480 1 license_plate, 6.9ms\n",
            "image 22/180 /content/dataset/images/val/175.jpg: 384x640 1 license_plate, 7.4ms\n",
            "image 23/180 /content/dataset/images/val/177.jpg: 640x480 1 license_plate, 7.2ms\n",
            "image 24/180 /content/dataset/images/val/186.jpg: 480x640 1 license_plate, 7.5ms\n",
            "image 25/180 /content/dataset/images/val/187.jpg: 480x640 1 license_plate, 7.6ms\n",
            "image 26/180 /content/dataset/images/val/198.jpg: 640x480 1 license_plate, 6.9ms\n",
            "image 27/180 /content/dataset/images/val/199.jpg: 480x640 1 license_plate, 7.1ms\n",
            "image 28/180 /content/dataset/images/val/207.jpg: 384x640 1 license_plate, 7.0ms\n",
            "image 29/180 /content/dataset/images/val/221.jpg: 480x640 1 license_plate, 7.1ms\n",
            "image 30/180 /content/dataset/images/val/222.jpg: 480x640 1 license_plate, 6.1ms\n",
            "image 31/180 /content/dataset/images/val/224.jpg: 480x640 1 license_plate, 6.6ms\n",
            "image 32/180 /content/dataset/images/val/226.jpg: 448x640 1 license_plate, 42.8ms\n",
            "image 33/180 /content/dataset/images/val/248.jpg: 480x640 1 license_plate, 7.0ms\n",
            "image 34/180 /content/dataset/images/val/250.jpg: 640x608 1 license_plate, 47.7ms\n",
            "image 35/180 /content/dataset/images/val/256.jpg: 640x512 1 license_plate, 41.1ms\n",
            "image 36/180 /content/dataset/images/val/272.jpg: 640x480 1 license_plate, 7.9ms\n",
            "image 37/180 /content/dataset/images/val/276.jpg: 384x640 1 license_plate, 7.0ms\n",
            "image 38/180 /content/dataset/images/val/278.jpg: 640x384 1 license_plate, 41.2ms\n",
            "image 39/180 /content/dataset/images/val/283.jpg: 384x640 1 license_plate, 6.8ms\n",
            "image 40/180 /content/dataset/images/val/287.jpg: 640x512 1 license_plate, 12.0ms\n",
            "image 41/180 /content/dataset/images/val/288.jpg: 352x640 1 license_plate, 7.1ms\n",
            "image 42/180 /content/dataset/images/val/289.jpg: 480x640 1 license_plate, 7.2ms\n",
            "image 43/180 /content/dataset/images/val/29.jpg: 384x640 1 license_plate, 7.4ms\n",
            "image 44/180 /content/dataset/images/val/291.jpg: 640x480 1 license_plate, 6.7ms\n",
            "image 45/180 /content/dataset/images/val/293.jpg: 512x640 1 license_plate, 43.1ms\n",
            "image 46/180 /content/dataset/images/val/296.jpg: 480x640 1 license_plate, 7.6ms\n",
            "image 47/180 /content/dataset/images/val/303.jpg: 640x608 1 license_plate, 7.9ms\n",
            "image 48/180 /content/dataset/images/val/307.jpg: 640x480 1 license_plate, 7.3ms\n",
            "image 49/180 /content/dataset/images/val/310.jpg: 480x640 1 license_plate, 8.0ms\n",
            "image 50/180 /content/dataset/images/val/314.jpg: 480x640 1 license_plate, 7.8ms\n",
            "image 51/180 /content/dataset/images/val/321.jpg: 384x640 1 license_plate, 7.2ms\n",
            "image 52/180 /content/dataset/images/val/324.jpg: 384x640 1 license_plate, 6.5ms\n",
            "image 53/180 /content/dataset/images/val/333.jpg: 480x640 1 license_plate, 7.4ms\n",
            "image 54/180 /content/dataset/images/val/338.jpg: 480x640 1 license_plate, 6.5ms\n",
            "image 55/180 /content/dataset/images/val/347.jpg: 384x640 1 license_plate, 7.9ms\n",
            "image 56/180 /content/dataset/images/val/351.jpg: 480x640 1 license_plate, 7.5ms\n",
            "image 57/180 /content/dataset/images/val/361.jpg: 480x640 1 license_plate, 6.6ms\n",
            "image 58/180 /content/dataset/images/val/364.jpg: 640x480 1 license_plate, 8.1ms\n",
            "image 59/180 /content/dataset/images/val/368.jpg: 480x640 1 license_plate, 7.4ms\n",
            "image 60/180 /content/dataset/images/val/369.jpg: 480x640 1 license_plate, 7.3ms\n",
            "image 61/180 /content/dataset/images/val/371.jpg: 448x640 1 license_plate, 7.4ms\n",
            "image 62/180 /content/dataset/images/val/378.jpg: 384x640 1 license_plate, 8.0ms\n",
            "image 63/180 /content/dataset/images/val/380.jpg: 384x640 1 license_plate, 6.2ms\n",
            "image 64/180 /content/dataset/images/val/382.jpg: 480x640 1 license_plate, 6.8ms\n",
            "image 65/180 /content/dataset/images/val/389.jpg: 480x640 1 license_plate, 6.2ms\n",
            "image 66/180 /content/dataset/images/val/390.jpg: 640x480 1 license_plate, 7.9ms\n",
            "image 67/180 /content/dataset/images/val/393.jpg: 384x640 1 license_plate, 7.4ms\n",
            "image 68/180 /content/dataset/images/val/395.jpg: 640x480 1 license_plate, 10.4ms\n",
            "image 69/180 /content/dataset/images/val/4.jpg: 640x480 1 license_plate, 6.2ms\n",
            "image 70/180 /content/dataset/images/val/403.jpg: 480x640 1 license_plate, 7.6ms\n",
            "image 71/180 /content/dataset/images/val/409.jpg: 352x640 1 license_plate, 6.7ms\n",
            "image 72/180 /content/dataset/images/val/410.jpg: 384x640 1 license_plate, 6.6ms\n",
            "image 73/180 /content/dataset/images/val/416.jpg: 640x480 1 license_plate, 7.0ms\n",
            "image 74/180 /content/dataset/images/val/423.jpg: 640x480 1 license_plate, 7.3ms\n",
            "image 75/180 /content/dataset/images/val/424.jpg: 480x640 1 license_plate, 7.1ms\n",
            "image 76/180 /content/dataset/images/val/426.jpg: 480x640 1 license_plate, 6.4ms\n",
            "image 77/180 /content/dataset/images/val/43.jpg: 640x480 1 license_plate, 7.1ms\n",
            "image 78/180 /content/dataset/images/val/432.jpg: 480x640 1 license_plate, 7.7ms\n",
            "image 79/180 /content/dataset/images/val/442.jpg: 640x480 1 license_plate, 6.6ms\n",
            "image 80/180 /content/dataset/images/val/450.jpg: 480x640 1 license_plate, 6.6ms\n",
            "image 81/180 /content/dataset/images/val/456.jpg: 480x640 1 license_plate, 6.2ms\n",
            "image 82/180 /content/dataset/images/val/458.jpg: 480x640 1 license_plate, 7.2ms\n",
            "image 83/180 /content/dataset/images/val/463.jpg: 640x480 1 license_plate, 9.6ms\n",
            "image 84/180 /content/dataset/images/val/464.jpg: 384x640 1 license_plate, 8.7ms\n",
            "image 85/180 /content/dataset/images/val/470.jpg: 480x640 1 license_plate, 7.3ms\n",
            "image 86/180 /content/dataset/images/val/48.jpg: 448x640 2 license_plates, 11.2ms\n",
            "image 87/180 /content/dataset/images/val/485.jpg: 640x480 1 license_plate, 14.0ms\n",
            "image 88/180 /content/dataset/images/val/487.jpg: 384x640 1 license_plate, 12.4ms\n",
            "image 89/180 /content/dataset/images/val/488.jpg: 384x640 1 license_plate, 11.5ms\n",
            "image 90/180 /content/dataset/images/val/496.jpg: 640x512 1 license_plate, 11.8ms\n",
            "image 91/180 /content/dataset/images/val/5.jpg: 448x640 1 license_plate, 14.2ms\n",
            "image 92/180 /content/dataset/images/val/501.jpg: 384x640 1 license_plate, 15.2ms\n",
            "image 93/180 /content/dataset/images/val/502.jpg: 480x640 1 license_plate, 9.0ms\n",
            "image 94/180 /content/dataset/images/val/503.jpg: 640x480 1 license_plate, 11.1ms\n",
            "image 95/180 /content/dataset/images/val/507.jpg: 640x480 1 license_plate, 8.1ms\n",
            "image 96/180 /content/dataset/images/val/515.jpg: 640x384 1 license_plate, 8.3ms\n",
            "image 97/180 /content/dataset/images/val/517.jpg: 640x480 1 license_plate, 10.9ms\n",
            "image 98/180 /content/dataset/images/val/518.jpg: 640x384 1 license_plate, 12.4ms\n",
            "image 99/180 /content/dataset/images/val/532.jpg: 480x640 1 license_plate, 9.4ms\n",
            "image 100/180 /content/dataset/images/val/536.jpg: 640x448 1 license_plate, 61.5ms\n",
            "image 101/180 /content/dataset/images/val/537.jpg: 384x640 1 license_plate, 16.4ms\n",
            "image 102/180 /content/dataset/images/val/538.jpg: 480x640 1 license_plate, 11.0ms\n",
            "image 103/180 /content/dataset/images/val/543.jpg: 480x640 1 license_plate, 9.2ms\n",
            "image 104/180 /content/dataset/images/val/544.jpg: 384x640 1 license_plate, 10.0ms\n",
            "image 105/180 /content/dataset/images/val/560.jpg: 512x640 1 license_plate, 9.1ms\n",
            "image 106/180 /content/dataset/images/val/561.jpg: 640x512 1 license_plate, 8.4ms\n",
            "image 107/180 /content/dataset/images/val/564.jpg: 640x384 1 license_plate, 9.0ms\n",
            "image 108/180 /content/dataset/images/val/566.jpg: 640x384 1 license_plate, 8.1ms\n",
            "image 109/180 /content/dataset/images/val/567.jpg: 384x640 1 license_plate, 9.1ms\n",
            "image 110/180 /content/dataset/images/val/57.jpg: 480x640 1 license_plate, 9.3ms\n",
            "image 111/180 /content/dataset/images/val/572.jpg: 384x640 1 license_plate, 9.0ms\n",
            "image 112/180 /content/dataset/images/val/578.jpg: 640x480 1 license_plate, 9.0ms\n",
            "image 113/180 /content/dataset/images/val/58.jpg: 480x640 1 license_plate, 9.8ms\n",
            "image 114/180 /content/dataset/images/val/582.jpg: 640x384 1 license_plate, 9.1ms\n",
            "image 115/180 /content/dataset/images/val/588.jpg: 640x480 1 license_plate, 11.4ms\n",
            "image 116/180 /content/dataset/images/val/589.jpg: 640x384 1 license_plate, 12.6ms\n",
            "image 117/180 /content/dataset/images/val/590.jpg: 640x480 1 license_plate, 10.7ms\n",
            "image 118/180 /content/dataset/images/val/594.jpg: 384x640 1 license_plate, 8.7ms\n",
            "image 119/180 /content/dataset/images/val/612.jpg: 384x640 1 license_plate, 9.2ms\n",
            "image 120/180 /content/dataset/images/val/617.jpg: 640x384 1 license_plate, 8.7ms\n",
            "image 121/180 /content/dataset/images/val/618.jpg: 640x384 1 license_plate, 8.7ms\n",
            "image 122/180 /content/dataset/images/val/62.jpg: 480x640 1 license_plate, 10.5ms\n",
            "image 123/180 /content/dataset/images/val/623.jpg: 640x416 1 license_plate, 60.5ms\n",
            "image 124/180 /content/dataset/images/val/624.jpg: 480x640 1 license_plate, 9.5ms\n",
            "image 125/180 /content/dataset/images/val/629.jpg: 480x640 1 license_plate, 8.5ms\n",
            "image 126/180 /content/dataset/images/val/63.jpg: 640x480 1 license_plate, 9.1ms\n",
            "image 127/180 /content/dataset/images/val/630.jpg: 640x480 1 license_plate, 8.2ms\n",
            "image 128/180 /content/dataset/images/val/642.jpg: 512x640 1 license_plate, 9.1ms\n",
            "image 129/180 /content/dataset/images/val/653.jpg: 640x480 1 license_plate, 9.4ms\n",
            "image 130/180 /content/dataset/images/val/662.jpg: 448x640 1 license_plate, 14.3ms\n",
            "image 131/180 /content/dataset/images/val/663.jpg: 640x480 1 license_plate, 10.5ms\n",
            "image 132/180 /content/dataset/images/val/668.jpg: 384x640 1 license_plate, 14.8ms\n",
            "image 133/180 /content/dataset/images/val/684.jpg: 640x384 1 license_plate, 12.2ms\n",
            "image 134/180 /content/dataset/images/val/687.jpg: 640x480 1 license_plate, 12.0ms\n",
            "image 135/180 /content/dataset/images/val/691.jpg: 384x640 1 license_plate, 12.8ms\n",
            "image 136/180 /content/dataset/images/val/692.jpg: 480x640 1 license_plate, 12.8ms\n",
            "image 137/180 /content/dataset/images/val/698.jpg: 640x480 1 license_plate, 10.0ms\n",
            "image 138/180 /content/dataset/images/val/700.jpg: 640x480 1 license_plate, 11.2ms\n",
            "image 139/180 /content/dataset/images/val/701.jpg: 640x480 1 license_plate, 9.3ms\n",
            "image 140/180 /content/dataset/images/val/705.jpg: 640x384 1 license_plate, 11.8ms\n",
            "image 141/180 /content/dataset/images/val/707.jpg: 384x640 1 license_plate, 11.1ms\n",
            "image 142/180 /content/dataset/images/val/719.jpg: 480x640 1 license_plate, 12.1ms\n",
            "image 143/180 /content/dataset/images/val/723.jpg: 384x640 1 license_plate, 12.4ms\n",
            "image 144/180 /content/dataset/images/val/738.jpg: 480x640 1 license_plate, 14.2ms\n",
            "image 145/180 /content/dataset/images/val/74.jpg: 480x640 1 license_plate, 14.9ms\n",
            "image 146/180 /content/dataset/images/val/758.jpg: 384x640 1 license_plate, 12.6ms\n",
            "image 147/180 /content/dataset/images/val/760.jpg: 480x640 1 license_plate, 10.6ms\n",
            "image 148/180 /content/dataset/images/val/765.jpg: 640x512 1 license_plate, 9.2ms\n",
            "image 149/180 /content/dataset/images/val/767.jpg: 448x640 1 license_plate, 8.9ms\n",
            "image 150/180 /content/dataset/images/val/769.jpg: 640x384 1 license_plate, 11.6ms\n",
            "image 151/180 /content/dataset/images/val/77.jpg: 512x640 1 license_plate, 10.6ms\n",
            "image 152/180 /content/dataset/images/val/770.jpg: 640x480 1 license_plate, 10.1ms\n",
            "image 153/180 /content/dataset/images/val/776.jpg: 480x640 1 license_plate, 8.7ms\n",
            "image 154/180 /content/dataset/images/val/780.jpg: 640x480 1 license_plate, 10.2ms\n",
            "image 155/180 /content/dataset/images/val/797.jpg: 480x640 1 license_plate, 10.8ms\n",
            "image 156/180 /content/dataset/images/val/8.jpg: 640x608 1 license_plate, 10.6ms\n",
            "image 157/180 /content/dataset/images/val/80.jpg: 384x640 1 license_plate, 10.3ms\n",
            "image 158/180 /content/dataset/images/val/800.jpg: 384x640 1 license_plate, 8.6ms\n",
            "image 159/180 /content/dataset/images/val/804.jpg: 384x640 1 license_plate, 8.4ms\n",
            "image 160/180 /content/dataset/images/val/81.jpg: 640x480 1 license_plate, 9.4ms\n",
            "image 161/180 /content/dataset/images/val/814.jpg: 480x640 1 license_plate, 9.2ms\n",
            "image 162/180 /content/dataset/images/val/82.jpg: 640x480 1 license_plate, 9.5ms\n",
            "image 163/180 /content/dataset/images/val/820.jpg: 640x480 1 license_plate, 8.2ms\n",
            "image 164/180 /content/dataset/images/val/826.jpg: 384x640 1 license_plate, 9.4ms\n",
            "image 165/180 /content/dataset/images/val/828.jpg: 640x384 1 license_plate, 10.7ms\n",
            "image 166/180 /content/dataset/images/val/838.jpg: 640x480 1 license_plate, 9.3ms\n",
            "image 167/180 /content/dataset/images/val/843.jpg: 480x640 2 license_plates, 9.2ms\n",
            "image 168/180 /content/dataset/images/val/844.jpg: 480x640 1 license_plate, 8.6ms\n",
            "image 169/180 /content/dataset/images/val/846.jpg: 640x480 1 license_plate, 9.1ms\n",
            "image 170/180 /content/dataset/images/val/849.jpg: 480x640 1 license_plate, 11.6ms\n",
            "image 171/180 /content/dataset/images/val/850.jpg: 640x544 1 license_plate, 110.2ms\n",
            "image 172/180 /content/dataset/images/val/853.jpg: 640x480 1 license_plate, 12.2ms\n",
            "image 173/180 /content/dataset/images/val/859.jpg: 384x640 1 license_plate, 33.7ms\n",
            "image 174/180 /content/dataset/images/val/871.jpg: 384x640 1 license_plate, 15.4ms\n",
            "image 175/180 /content/dataset/images/val/887.jpg: 640x512 1 license_plate, 13.8ms\n",
            "image 176/180 /content/dataset/images/val/897.jpg: 640x384 1 license_plate, 37.7ms\n",
            "image 177/180 /content/dataset/images/val/91.jpg: 512x640 1 license_plate, 24.0ms\n",
            "image 178/180 /content/dataset/images/val/94.jpg: 480x640 1 license_plate, 16.1ms\n",
            "image 179/180 /content/dataset/images/val/95.jpg: 480x640 1 license_plate, 27.8ms\n",
            "image 180/180 /content/dataset/images/val/97.jpg: 352x640 1 license_plate, 36.8ms\n",
            "Speed: 3.1ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            " Predictions saved in: /content/runs/detect/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "output_crop_dir = \"/content/cropped_predicted\"\n",
        "os.makedirs(output_crop_dir, exist_ok=True)\n",
        "\n",
        "for r in results:\n",
        "    img = cv2.imread(r.path)\n",
        "    for i, box in enumerate(r.boxes.xyxy.tolist()):  # xyxy = [xmin, ymin, xmax, ymax]\n",
        "        xmin, ymin, xmax, ymax = map(int, box)\n",
        "        crop = img[ymin:ymax, xmin:xmax]\n",
        "        out_path = os.path.join(output_crop_dir, f\"{os.path.basename(r.path)}_plate{i}.jpg\")\n",
        "        cv2.imwrite(out_path, crop)\n",
        "\n",
        "print(\" Cropped plates saved in:\", output_crop_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7AzrHpOftL7",
        "outputId": "ea9ec898-dde5-4cae-d46f-add3b8138d2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cropped plates saved in: /content/cropped_predicted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/datasets/Licplatesrecognition_train.zip\"\n",
        "extract_path = \"/content/plates_recognition_unzipped\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Done! Files extracted to:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_YSHVE5RaU_",
        "outputId": "98bc6aba-fbe5-45d5-c8cf-902ca4c74e51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Files extracted to: /content/plates_recognition_unzipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "csv_path = \"/content/drive/MyDrive/datasets/Licplatesrecognition_train1.csv\"   # your CSV with (img_id, text)\n",
        "images_dir = \"/content/plates_recognition_unzipped/license_plates_recognition_train\"          # folder with cropped license plate images\n",
        "output_dir = \"/content/ocr_annotations\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Train/val split (80/20)\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "def save_txt(df, out_file):\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(images_dir, str(row[\"img_id\"]))\n",
        "            text = str(row[\"text\"]).strip()\n",
        "            f.write(f\"{img_path}\\t{text}\\n\")\n",
        "\n",
        "# Save files\n",
        "save_txt(train_df, os.path.join(output_dir, \"train.txt\"))\n",
        "save_txt(val_df, os.path.join(output_dir, \"val.txt\"))\n",
        "\n",
        "print(\"Train/Val annotation files created in:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kink_GOrhI4M",
        "outputId": "8fc57a87-b7ac-4cd3-e117-fd0857564210"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val annotation files created in: /content/ocr_annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > /content/train.py <<'PY'\n",
        "# train.py  — CRNN+CTC training for license plates (digits + 'T')\n",
        "# Works with a CSV that has columns: img_id, text\n",
        "# Example call:\n",
        "#   python3 train.py \\\n",
        "#     --img_dir \"/content/plates_recognition_unzipped/license_plates_recognition_train\" \\\n",
        "#     --ann \"/content/drive/MyDrive/datasets/Licplatesrecognition_train1.csv\" \\\n",
        "#     --charset \"0123456789T\" \\\n",
        "#     --epochs 20 --batch 16 --out \"/content/ocr_model.pth\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "def seed_everything(seed: int = 2024):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def robust_read_annotations(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Tries to read CSV with headers (img_id,text). If that fails,\n",
        "    tries tab-separated without header and renames to (img_id,text).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        if not {\"img_id\", \"text\"}.issubset(df.columns):\n",
        "            raise ValueError(\"Columns not found\")\n",
        "        return df[[\"img_id\", \"text\"]]\n",
        "    except Exception:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", header=None, engine=\"python\")\n",
        "        if df.shape[1] < 2:\n",
        "            raise ValueError(\"Annotation file must have at least two columns\")\n",
        "        df = df.iloc[:, :2]\n",
        "        df.columns = [\"img_id\", \"text\"]\n",
        "        return df\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset\n",
        "# ----------------------------\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, img_dir: str, ann_path: str, charset: str, transform=None):\n",
        "        \"\"\"\n",
        "        img_dir: base folder with images (ignored if img_id is absolute path)\n",
        "        ann_path: CSV/TSV with columns [img_id, text]\n",
        "        charset: string of characters to recognize (blank handled separately)\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.df = robust_read_annotations(ann_path)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.charset = charset\n",
        "        self.blank_idx = 0\n",
        "        self.char2idx = {c: i + 1 for i, c in enumerate(self.charset)}  # 0 reserved for CTC blank\n",
        "        self.idx2char = {i + 1: c for i, c in enumerate(self.charset)}\n",
        "\n",
        "        # Filter rows with empty text (CTC needs label length > 0)\n",
        "        before = len(self.df)\n",
        "        self.df = self.df[self.df[\"text\"].astype(str).str.len() > 0].reset_index(drop=True)\n",
        "        after = len(self.df)\n",
        "        if after < before:\n",
        "            print(f\"[WARN] Dropped {before - after} samples with empty labels.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _resolve_path(self, img_id: str) -> str:\n",
        "        if os.path.isabs(img_id) and os.path.exists(img_id):\n",
        "            return img_id\n",
        "        p = os.path.join(self.img_dir, img_id)\n",
        "        return p\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        row = self.df.iloc[idx]\n",
        "        img_id, label_str = str(row[\"img_id\"]), str(row[\"text\"])\n",
        "        img_path = self._resolve_path(img_id)\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"L\")  # grayscale\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Encode label -> indices (skip chars not in charset)\n",
        "        label_idx = [self.char2idx[c] for c in label_str if c in self.char2idx]\n",
        "        if len(label_idx) == 0:\n",
        "            # Ensure non-empty for CTC; if empty, put a dummy (will be ignored statistically)\n",
        "            # but better to have no empty labels in data.\n",
        "            label_idx = [self.char2idx[self.charset[0]]]\n",
        "        return img, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n",
        "\n",
        "def ctc_collate_fn(batch):\n",
        "    imgs, labels = zip(*batch)  # lists\n",
        "    imgs = torch.stack(imgs, dim=0)  # (N, 1, H, W)\n",
        "\n",
        "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
        "    labels = torch.cat(labels, dim=0)  # 1D concat of all labels\n",
        "    return imgs, labels, label_lengths\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CRNN Model (VGG-like CNN + 2xBiLSTM + Linear)\n",
        "# ----------------------------\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, num_classes: int, img_h: int = 32):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes  # includes CTC blank at index 0\n",
        "\n",
        "        def conv_block(in_c, out_c, k=3, s=1, p=1, bn=True):\n",
        "            layers = [nn.Conv2d(in_c, out_c, k, s, p), nn.ReLU(True)]\n",
        "            if bn:\n",
        "                layers.insert(1, nn.BatchNorm2d(out_c))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        # CNN: reduce H to 1; preserve width as \"time\"\n",
        "        self.cnn = nn.Sequential(\n",
        "            conv_block(1, 64, bn=False),          # -> (64, H, W)\n",
        "            nn.MaxPool2d(2, 2),                   # -> (64, H/2, W/2)\n",
        "\n",
        "            conv_block(64, 128, bn=False),        # -> (128, H/2, W/2)\n",
        "            nn.MaxPool2d(2, 2),                   # -> (128, H/4, W/4)\n",
        "\n",
        "            conv_block(128, 256),                 # -> (256, H/4, W/4)\n",
        "            conv_block(256, 256),                 # -> (256, H/4, W/4)\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # -> (256, H/8, W/4)\n",
        "\n",
        "            conv_block(256, 512),                 # -> (512, H/8, W/4)\n",
        "            conv_block(512, 512),                 # -> (512, H/8, W/4)\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # -> (512, H/16, W/4)\n",
        "\n",
        "            # Reduce height to 1\n",
        "            nn.Conv2d(512, 512, kernel_size=(img_h // 16, 3), stride=1, padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # After CNN: (N, 512, 1, W') -> sequence len = W'\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=False,  # we feed (T, N, C)\n",
        "        )\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, 1, H, W)\n",
        "        feats = self.cnn(x)            # (N, 512, 1, W')\n",
        "        feats = feats.squeeze(2)       # (N, 512, W')\n",
        "        feats = feats.permute(2, 0, 1) # (T=W', N, 512)\n",
        "        y, _ = self.rnn(feats)         # (T, N, 512)\n",
        "        y = self.fc(y)                 # (T, N, num_classes)\n",
        "        return y\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Greedy CTC decoder (for quick sanity checks)\n",
        "# ----------------------------\n",
        "def ctc_greedy_decode(logits: torch.Tensor, idx2char: dict, blank_idx: int = 0) -> List[str]:\n",
        "    \"\"\"\n",
        "    logits: (T, N, C) – raw (not softmaxed) or log-probs\n",
        "    \"\"\"\n",
        "    probs = logits.detach().cpu().softmax(2)\n",
        "    best = probs.argmax(2)  # (T, N)\n",
        "    T, N = best.shape\n",
        "    texts = []\n",
        "    for n in range(N):\n",
        "        prev = None\n",
        "        s = []\n",
        "        for t in range(T):\n",
        "            k = best[t, n].item()\n",
        "            if k != blank_idx and k != prev:\n",
        "                if k in idx2char:\n",
        "                    s.append(idx2char[k])\n",
        "            prev = k\n",
        "        texts.append(\"\".join(s))\n",
        "    return texts\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Train\n",
        "# ----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--img_dir\", type=str, required=True,\n",
        "                        help=\"Folder containing plate images (ignored if img_id has absolute paths)\")\n",
        "    parser.add_argument(\"--ann\", type=str, required=True,\n",
        "                        help=\"CSV/TSV with columns img_id,text (or tab-separated two columns)\")\n",
        "    parser.add_argument(\"--charset\", type=str, default=\"0123456789T\",\n",
        "                        help=\"Characters to recognize (CTC blank is auto-added)\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "    parser.add_argument(\"--batch\", type=int, default=16)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--height\", type=int, default=32)\n",
        "    parser.add_argument(\"--width\", type=int, default=128)\n",
        "    parser.add_argument(\"--workers\", type=int, default=2)\n",
        "    parser.add_argument(\"--out\", type=str, default=\"ocr_model.pth\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=2024)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Args:\", vars(args))\n",
        "    seed_everything(args.seed)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((args.height, args.width)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "    ])\n",
        "\n",
        "    dataset = OCRDataset(args.img_dir, args.ann, args.charset, transform=transform)\n",
        "    print(f\"Dataset size: {len(dataset)} images\")\n",
        "    print(f\"Charset: {args.charset} (len={len(args.charset)})\")\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=(device == \"cuda\"),\n",
        "        collate_fn=ctc_collate_fn,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    num_classes = len(args.charset) + 1  # +1 for CTC blank at index 0\n",
        "    model = CRNN(num_classes=num_classes, img_h=args.height).to(device)\n",
        "\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for step, (imgs, labels, label_lengths) in enumerate(loader, start=1):\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(imgs)  # (T, N, C)\n",
        "            log_probs = F.log_softmax(logits, dim=2)\n",
        "\n",
        "            T, N, C = log_probs.shape\n",
        "            pred_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long, device=log_probs.device)\n",
        "\n",
        "            loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if step % 50 == 0 or step == 1:\n",
        "                # quick decode preview for first few samples\n",
        "                with torch.no_grad():\n",
        "                    sample_txt = ctc_greedy_decode(logits[:, : min(2, N), :], dataset.idx2char, blank_idx=0)\n",
        "                print(f\"Epoch {epoch}/{args.epochs} | Step {step}/{len(loader)} | \"\n",
        "                      f\"Loss {loss.item():.4f} | Preview: {sample_txt}\")\n",
        "\n",
        "        avg = total_loss / max(1, len(loader))\n",
        "        print(f\"==> Epoch {epoch} done. Avg Loss: {avg:.4f}\")\n",
        "\n",
        "    # Save model + charset used\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"charset\": args.charset,\n",
        "        \"img_h\": args.height,\n",
        "        \"img_w\": args.width,\n",
        "    }, args.out)\n",
        "    with open(os.path.splitext(args.out)[0] + \"_charset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(args.charset)\n",
        "\n",
        "    print(f\"✅ Training finished. Model saved to: {args.out}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "PY\n"
      ],
      "metadata": {
        "id": "wpDlFWyYsFUL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/train.py \\\n",
        "  --img_dir \"/content/plates_recognition_unzipped/license_plates_recognition_train\" \\\n",
        "  --ann \"/content/drive/MyDrive/datasets/Licplatesrecognition_train1.csv\" \\\n",
        "  --charset \"0123456789T\" \\\n",
        "  --epochs 20 \\\n",
        "  --batch 16 \\\n",
        "  --out \"/content/ocr_model.pth\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFxW3xkAsVc0",
        "outputId": "9286011a-938e-4b41-b08e-8fddf3718c5d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args: {'img_dir': '/content/plates_recognition_unzipped/license_plates_recognition_train', 'ann': '/content/drive/MyDrive/datasets/Licplatesrecognition_train1.csv', 'charset': '0123456789T', 'epochs': 20, 'batch': 16, 'lr': 0.001, 'height': 32, 'width': 128, 'workers': 2, 'out': '/content/ocr_model.pth', 'seed': 2024}\n",
            "Device: cuda\n",
            "Dataset size: 900 images\n",
            "Charset: 0123456789T (len=11)\n",
            "Epoch 1/20 | Step 1/57 | Loss 7.1445 | Preview: ['681565757', '16561651']\n",
            "Epoch 1/20 | Step 50/57 | Loss 2.4899 | Preview: ['', '']\n",
            "==> Epoch 1 done. Avg Loss: 2.6745\n",
            "Epoch 2/20 | Step 1/57 | Loss 2.5051 | Preview: ['1', '1']\n",
            "Epoch 2/20 | Step 50/57 | Loss 2.4164 | Preview: ['1', '1']\n",
            "==> Epoch 2 done. Avg Loss: 2.4086\n",
            "Epoch 3/20 | Step 1/57 | Loss 2.4078 | Preview: ['1', '1']\n",
            "Epoch 3/20 | Step 50/57 | Loss 2.3755 | Preview: ['1', '1']\n",
            "==> Epoch 3 done. Avg Loss: 2.3512\n",
            "Epoch 4/20 | Step 1/57 | Loss 2.3308 | Preview: ['1', '1']\n",
            "Epoch 4/20 | Step 50/57 | Loss 2.2766 | Preview: ['1', '1']\n",
            "==> Epoch 4 done. Avg Loss: 2.3098\n",
            "Epoch 5/20 | Step 1/57 | Loss 2.3151 | Preview: ['1', '1']\n",
            "Epoch 5/20 | Step 50/57 | Loss 2.0858 | Preview: ['1T', '1']\n",
            "==> Epoch 5 done. Avg Loss: 2.2251\n",
            "Epoch 6/20 | Step 1/57 | Loss 2.0608 | Preview: ['11T', '1T']\n",
            "Epoch 6/20 | Step 50/57 | Loss 2.6948 | Preview: ['11T', '9T']\n",
            "==> Epoch 6 done. Avg Loss: 2.1808\n",
            "Epoch 7/20 | Step 1/57 | Loss 2.1533 | Preview: ['1T', '1T']\n",
            "Epoch 7/20 | Step 50/57 | Loss 1.9369 | Preview: ['1T', '1T']\n",
            "==> Epoch 7 done. Avg Loss: 2.0734\n",
            "Epoch 8/20 | Step 1/57 | Loss 2.4899 | Preview: ['1T', '9T']\n",
            "Epoch 8/20 | Step 50/57 | Loss 1.9347 | Preview: ['1T', '10T0']\n",
            "==> Epoch 8 done. Avg Loss: 2.0039\n",
            "Epoch 9/20 | Step 1/57 | Loss 1.9408 | Preview: ['16T', '1T1']\n",
            "Epoch 9/20 | Step 50/57 | Loss 1.8718 | Preview: ['1T', '9T']\n",
            "==> Epoch 9 done. Avg Loss: 1.9458\n",
            "Epoch 10/20 | Step 1/57 | Loss 1.9463 | Preview: ['1T', '15T']\n",
            "Epoch 10/20 | Step 50/57 | Loss 2.2330 | Preview: ['161T', '16T1']\n",
            "==> Epoch 10 done. Avg Loss: 1.9028\n",
            "Epoch 11/20 | Step 1/57 | Loss 1.8510 | Preview: ['10T', '1T']\n",
            "Epoch 11/20 | Step 50/57 | Loss 1.8373 | Preview: ['9T3', '16T10']\n",
            "==> Epoch 11 done. Avg Loss: 1.7826\n",
            "Epoch 12/20 | Step 1/57 | Loss 1.7672 | Preview: ['141T93', '14T']\n",
            "Epoch 12/20 | Step 50/57 | Loss 1.4994 | Preview: ['161T39', '16T3']\n",
            "==> Epoch 12 done. Avg Loss: 1.7204\n",
            "Epoch 13/20 | Step 1/57 | Loss 1.6193 | Preview: ['111T', '112T4']\n",
            "Epoch 13/20 | Step 50/57 | Loss 1.5866 | Preview: ['12T7', '120T104']\n",
            "==> Epoch 13 done. Avg Loss: 1.5933\n",
            "Epoch 14/20 | Step 1/57 | Loss 1.6350 | Preview: ['99T13', '73T330']\n",
            "Epoch 14/20 | Step 50/57 | Loss 1.7110 | Preview: ['129T333', '16T971']\n",
            "==> Epoch 14 done. Avg Loss: 1.4287\n",
            "Epoch 15/20 | Step 1/57 | Loss 1.2044 | Preview: ['89T14', '11T4']\n",
            "Epoch 15/20 | Step 50/57 | Loss 1.1343 | Preview: ['15T846', '16T7317']\n",
            "==> Epoch 15 done. Avg Loss: 1.1223\n",
            "Epoch 16/20 | Step 1/57 | Loss 1.0982 | Preview: ['25T13044', '89T3403']\n",
            "Epoch 16/20 | Step 50/57 | Loss 0.5563 | Preview: ['107T541', '10369']\n",
            "==> Epoch 16 done. Avg Loss: 0.7830\n",
            "Epoch 17/20 | Step 1/57 | Loss 0.5650 | Preview: ['72T3306', '182T6104']\n",
            "Epoch 17/20 | Step 50/57 | Loss 0.3054 | Preview: ['155T9278', '120T5039']\n",
            "==> Epoch 17 done. Avg Loss: 0.4823\n",
            "Epoch 18/20 | Step 1/57 | Loss 0.4228 | Preview: ['144T9065', '194T5474']\n",
            "Epoch 18/20 | Step 50/57 | Loss 0.2296 | Preview: ['140T659', '146T6543']\n",
            "==> Epoch 18 done. Avg Loss: 0.3357\n",
            "Epoch 19/20 | Step 1/57 | Loss 0.4822 | Preview: ['1T70', '138T1769']\n",
            "Epoch 19/20 | Step 50/57 | Loss 0.1601 | Preview: ['107T541', '86T564']\n",
            "==> Epoch 19 done. Avg Loss: 0.2996\n",
            "Epoch 20/20 | Step 1/57 | Loss 0.1910 | Preview: ['95T7304', '107T5823']\n",
            "Epoch 20/20 | Step 50/57 | Loss 0.2969 | Preview: ['104T489', '39T5898']\n",
            "==> Epoch 20 done. Avg Loss: 0.2165\n",
            "✅ Training finished. Model saved to: /content/ocr_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/datasets/test.zip\"\n",
        "extract_path = \"/content/test_unzipped\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Done! Files extracted to:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf86ROFgt8NF",
        "outputId": "2ea354a9-0cfa-47aa-dca7-c38314513317"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Files extracted to: /content/test_unzipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "# ------------------------\n",
        "# CRNN model (same as train.py)\n",
        "# ------------------------\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, num_classes: int, img_h: int = 32):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        def conv_block(in_c, out_c, k=3, s=1, p=1, bn=True):\n",
        "            layers = [nn.Conv2d(in_c, out_c, k, s, p), nn.ReLU(True)]\n",
        "            if bn: layers.insert(1, nn.BatchNorm2d(out_c))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            conv_block(1, 64, bn=False), nn.MaxPool2d(2, 2),\n",
        "            conv_block(64, 128, bn=False), nn.MaxPool2d(2, 2),\n",
        "            conv_block(128, 256), conv_block(256, 256),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
        "            conv_block(256, 512), conv_block(512, 512),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
        "            nn.Conv2d(512, 512, kernel_size=(img_h // 16, 3), stride=1, padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.rnn = nn.LSTM(input_size=512, hidden_size=256, num_layers=2,\n",
        "                           bidirectional=True, batch_first=False)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.cnn(x)\n",
        "        feats = feats.squeeze(2)\n",
        "        feats = feats.permute(2, 0, 1)\n",
        "        y, _ = self.rnn(feats)\n",
        "        y = self.fc(y)\n",
        "        return y\n",
        "\n",
        "# ------------------------\n",
        "# Load model checkpoint\n",
        "# ------------------------\n",
        "checkpoint_path = \"/content/ocr_model.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "charset = checkpoint['charset']\n",
        "img_h = checkpoint['img_h']\n",
        "img_w = checkpoint['img_w']\n",
        "\n",
        "num_classes = len(charset) + 1\n",
        "model = CRNN(num_classes=num_classes, img_h=img_h)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# ------------------------\n",
        "# Preprocessing function\n",
        "# ------------------------\n",
        "def preprocess_image(img_path, img_h, img_w, enhance_contrast=True, sharpen=False):\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "    # Contrast enhancement\n",
        "    if enhance_contrast:\n",
        "        img = ImageEnhance.Contrast(img).enhance(2.0)\n",
        "\n",
        "    # Optional sharpening\n",
        "    if sharpen:\n",
        "        img = img.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "    # Resize and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_h, img_w)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "    return transform(img).unsqueeze(0)  # add batch dimension\n",
        "\n",
        "# ------------------------\n",
        "# Greedy CTC decoding\n",
        "# ------------------------\n",
        "def ctc_greedy_decode(logits, idx2char, blank_idx=0):\n",
        "    probs = logits.detach().cpu().softmax(2)\n",
        "    best = probs.argmax(2)  # (T, N)\n",
        "    T, N = best.shape\n",
        "    texts = []\n",
        "    for n in range(N):\n",
        "        prev = None\n",
        "        s = []\n",
        "        for t in range(T):\n",
        "            k = best[t, n].item()\n",
        "            if k != blank_idx and k != prev:\n",
        "                if k in idx2char:\n",
        "                    s.append(idx2char[k])\n",
        "            prev = k\n",
        "        texts.append(\"\".join(s))\n",
        "    return texts\n",
        "\n",
        "# ------------------------\n",
        "# Run inference on a folder\n",
        "# ------------------------\n",
        "test_folder = \"/content/test_unzipped/test/test/\"\n",
        "test_images = list(Path(test_folder).glob(\"*.jpg\"))\n",
        "idx2char = {i+1: c for i, c in enumerate(charset)}\n",
        "\n",
        "results_dict = {}\n",
        "for img_path in test_images:\n",
        "    img_tensor = preprocess_image(img_path, img_h, img_w, enhance_contrast=True, sharpen=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(img_tensor)\n",
        "    pred_text = ctc_greedy_decode(logits, idx2char)[0]\n",
        "    results_dict[img_path.name] = pred_text\n",
        "    print(f\"{img_path.name}: {pred_text}\")\n",
        "\n",
        "# Save predictions\n",
        "df = pd.DataFrame([(k, v) for k, v in results_dict.items()],\n",
        "                  columns=['image', 'predicted_text'])\n",
        "df.to_csv(\"/content/ocr_predictions_enhanced.csv\", index=False)\n",
        "print(\"✅ Enhanced predictions saved to /content/ocr_predictions_enhanced.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hHMI6hmt4Ta",
        "outputId": "4093acef-2a2d-4cc3-cd1c-03fb71b1117e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "926.jpg: 1T57\n",
            "1036.jpg: 6T05\n",
            "1033.jpg: 7T73\n",
            "911.jpg: 4T1\n",
            "933.jpg: T0\n",
            "977.jpg: T7\n",
            "1068.jpg: 14T7\n",
            "1046.jpg: T08\n",
            "992.jpg: 8T\n",
            "1084.jpg: 1T44\n",
            "1038.jpg: 1T\n",
            "1019.jpg: 2T57\n",
            "981.jpg: T4\n",
            "1051.jpg: 7T\n",
            "934.jpg: 1T45\n",
            "1008.jpg: 0T0\n",
            "989.jpg: 70T254\n",
            "906.jpg: 7T8\n",
            "1034.jpg: 72T8\n",
            "930.jpg: 7T\n",
            "990.jpg: 76T7\n",
            "996.jpg: 3T9\n",
            "952.jpg: T69\n",
            "943.jpg: 4T971\n",
            "963.jpg: 5T3\n",
            "1077.jpg: T57\n",
            "1035.jpg: 14T132\n",
            "994.jpg: 0T\n",
            "1111.jpg: 2T0\n",
            "1040.jpg: 1T9\n",
            "965.jpg: 1T2\n",
            "1003.jpg: 39T1\n",
            "958.jpg: 7T47\n",
            "1009.jpg: 17T9\n",
            "932.jpg: 9T\n",
            "973.jpg: 7T\n",
            "901.jpg: T57\n",
            "954.jpg: 1T7\n",
            "936.jpg: 4T1\n",
            "1062.jpg: 10T34\n",
            "924.jpg: 2T\n",
            "914.jpg: 18T7\n",
            "1099.jpg: 14T754\n",
            "1018.jpg: 1T977\n",
            "993.jpg: 1T\n",
            "1102.jpg: 70T5\n",
            "1014.jpg: 50T84\n",
            "971.jpg: 7T\n",
            "902.jpg: 54T67\n",
            "1030.jpg: 1T805\n",
            "1069.jpg: T\n",
            "982.jpg: 1T07\n",
            "1098.jpg: 5T3\n",
            "927.jpg: 172T2\n",
            "940.jpg: T74\n",
            "1057.jpg: 8T9\n",
            "950.jpg: 3T447\n",
            "1082.jpg: 7T7\n",
            "1101.jpg: 7T7\n",
            "1002.jpg: 8T7\n",
            "918.jpg: 13T12\n",
            "1022.jpg: T42\n",
            "1024.jpg: 5T\n",
            "960.jpg: T75\n",
            "1086.jpg: 0T\n",
            "1011.jpg: 0T7\n",
            "1005.jpg: 1T4\n",
            "1037.jpg: 141T7\n",
            "1083.jpg: 1T\n",
            "1110.jpg: 8T9\n",
            "986.jpg: T1\n",
            "957.jpg: T7\n",
            "917.jpg: 1T71\n",
            "983.jpg: T21\n",
            "962.jpg: T1\n",
            "945.jpg: 10T00\n",
            "1054.jpg: 87T\n",
            "1007.jpg: 1T9\n",
            "995.jpg: 16T4\n",
            "991.jpg: T574\n",
            "966.jpg: T11\n",
            "1043.jpg: 8T9\n",
            "1039.jpg: 512\n",
            "1052.jpg: 5T4477\n",
            "1090.jpg: 2T49\n",
            "998.jpg: 17T\n",
            "1105.jpg: 7T9\n",
            "1015.jpg: 7T72\n",
            "942.jpg: 1T5\n",
            "1067.jpg: 17T63\n",
            "1047.jpg: 0T42\n",
            "937.jpg: 1T22\n",
            "1079.jpg: T957\n",
            "1109.jpg: 1T\n",
            "951.jpg: 1T73\n",
            "1016.jpg: 12T2\n",
            "941.jpg: T3\n",
            "938.jpg: 1T7\n",
            "912.jpg: 107T141\n",
            "1056.jpg: 15T\n",
            "1094.jpg: T22\n",
            "1070.jpg: T\n",
            "961.jpg: 7T27\n",
            "1078.jpg: 7T73\n",
            "970.jpg: 1T2\n",
            "1044.jpg: T199\n",
            "1013.jpg: 4T920\n",
            "1092.jpg: 77T737\n",
            "919.jpg: 14T37\n",
            "988.jpg: 80T\n",
            "923.jpg: 1T2\n",
            "956.jpg: 3T\n",
            "1041.jpg: 7T1\n",
            "1050.jpg: 1T9057\n",
            "1061.jpg: 0T\n",
            "1095.jpg: T0\n",
            "1107.jpg: 2T821\n",
            "1064.jpg: T\n",
            "1028.jpg: 1T28\n",
            "1023.jpg: 0T274\n",
            "1053.jpg: T0\n",
            "984.jpg: 8T293\n",
            "1081.jpg: 8T70\n",
            "1073.jpg: 1T71\n",
            "1045.jpg: 7T\n",
            "947.jpg: 1T5\n",
            "909.jpg: 14T\n",
            "1072.jpg: 30T207\n",
            "999.jpg: 1T\n",
            "1055.jpg: T\n",
            "974.jpg: 7T\n",
            "1088.jpg: 2T9\n",
            "1106.jpg: 1T91\n",
            "1076.jpg: 1T\n",
            "1074.jpg: 17T77304\n",
            "1031.jpg: 14TT4\n",
            "1020.jpg: 67T\n",
            "1017.jpg: 79T42\n",
            "1026.jpg: 17T\n",
            "948.jpg: T55\n",
            "1075.jpg: 1T7\n",
            "928.jpg: 1T23\n",
            "1100.jpg: 0T7\n",
            "1091.jpg: 4T10\n",
            "920.jpg: 3T00\n",
            "1042.jpg: 7T21\n",
            "905.jpg: 7T01\n",
            "908.jpg: 14T420\n",
            "967.jpg: 1T7\n",
            "1021.jpg: 47T\n",
            "1000.jpg: 8T2041\n",
            "997.jpg: 14T04\n",
            "964.jpg: T7\n",
            "1059.jpg: 1T7\n",
            "1108.jpg: 14T7\n",
            "978.jpg: 1T87\n",
            "946.jpg: 17T5\n",
            "925.jpg: 4T47\n",
            "1006.jpg: 49T\n",
            "1048.jpg: 72T43\n",
            "979.jpg: T1\n",
            "975.jpg: 4T47\n",
            "1113.jpg: 2T7\n",
            "968.jpg: 17T\n",
            "1096.jpg: 4T31\n",
            "1032.jpg: 24T10\n",
            "904.jpg: 1T\n",
            "939.jpg: 1T4\n",
            "1010.jpg: 2T30\n",
            "1080.jpg: T\n",
            "987.jpg: 74T847\n",
            "1029.jpg: 1T7\n",
            "976.jpg: 6T\n",
            "913.jpg: 8T43\n",
            "969.jpg: T\n",
            "944.jpg: T80\n",
            "1071.jpg: 14T8\n",
            "907.jpg: 17T504\n",
            "1065.jpg: 1T9\n",
            "1103.jpg: 6T05\n",
            "1066.jpg: 14T5\n",
            "910.jpg: 4T74781\n",
            "1089.jpg: 97T21\n",
            "972.jpg: 1T717\n",
            "915.jpg: 17T479\n",
            "1001.jpg: T8\n",
            "1085.jpg: 82T07\n",
            "1025.jpg: 4T4\n",
            "953.jpg: 7T1\n",
            "1063.jpg: 15T0\n",
            "985.jpg: 7T3\n",
            "1049.jpg: 1T\n",
            "931.jpg: 1T234\n",
            "1004.jpg: 1T\n",
            "1093.jpg: 1T\n",
            "935.jpg: 6T0\n",
            "955.jpg: T47\n",
            "1104.jpg: 5T1410\n",
            "1027.jpg: 0T749\n",
            "949.jpg: 8T04\n",
            "922.jpg: 9T2\n",
            "959.jpg: 7T9\n",
            "929.jpg: T1\n",
            "903.jpg: 10T57\n",
            "1012.jpg: 17T44\n",
            "1058.jpg: 4T10\n",
            "921.jpg: 15T27\n",
            "1112.jpg: 9T79\n",
            "1087.jpg: 4T003\n",
            "1060.jpg: 1T\n",
            "✅ Enhanced predictions saved to /content/ocr_predictions_enhanced.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load raw predictions\n",
        "df = pd.read_csv(\"/content/ocr_predictions.csv\")\n",
        "\n",
        "# Define allowed charset\n",
        "allowed_chars = \"0123456789T\"\n",
        "\n",
        "def clean_plate(text):\n",
        "    # Keep only allowed characters\n",
        "    cleaned = \"\".join([c for c in text if c in allowed_chars])\n",
        "\n",
        "    # Optional: ensure at most one 'T' (common in your dataset)\n",
        "    if cleaned.count('T') > 1:\n",
        "        # Keep first 'T', remove others\n",
        "        first_T = cleaned.index('T')\n",
        "        cleaned = cleaned[:first_T+1] + cleaned[first_T+1:].replace('T','')\n",
        "\n",
        "    # Optional: basic pattern enforcement\n",
        "    # Example: at least 2 characters, max 7\n",
        "    if len(cleaned) < 2:\n",
        "        cleaned = cleaned + \"0\"*(2-len(cleaned))\n",
        "    elif len(cleaned) > 7:\n",
        "        cleaned = cleaned[:7]\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# Apply cleaning\n",
        "df['predicted_text_clean'] = df['predicted_text'].apply(clean_plate)\n",
        "\n",
        "# Save cleaned predictions\n",
        "df.to_csv(\"/content/ocr_predictions_clean.csv\", index=False)\n",
        "print(\"✅ Cleaned predictions saved to /content/ocr_predictions_clean.csv\")\n",
        "print(df[['image','predicted_text','predicted_text_clean']].head(15))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9QfTCWIxUq8",
        "outputId": "b23bdc23-8f2e-4fe7-ff6d-e11b50d7c880"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned predictions saved to /content/ocr_predictions_clean.csv\n",
            "       image predicted_text predicted_text_clean\n",
            "0    926.jpg            T57                  T57\n",
            "1   1036.jpg            1T9                  1T9\n",
            "2   1033.jpg            2T9                  2T9\n",
            "3    911.jpg            1T1                  1T1\n",
            "4    933.jpg             T0                   T0\n",
            "5    977.jpg             T2                   T2\n",
            "6   1068.jpg         17T777               17T777\n",
            "7   1046.jpg            1T9                  1T9\n",
            "8    992.jpg              T                   T0\n",
            "9   1084.jpg             1T                   1T\n",
            "10  1038.jpg             1T                   1T\n",
            "11  1019.jpg            2T2                  2T2\n",
            "12   981.jpg            T42                  T42\n",
            "13  1051.jpg            7T4                  7T4\n",
            "14   934.jpg          14T45                14T45\n"
          ]
        }
      ]
    }
  ]
}